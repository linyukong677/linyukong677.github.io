---
layout: post
title: Bayesian in Machine Learning
date: 2022-11-03 17:13 +0800
last_modified_at: 2022-11-04 19:08 +0800
tags: [Bayesian, Machine Learning]
toc:  true
math: true
---

现在很多Machine Learning的入门课程都不会深入介绍Bayesian方法了，其实是一件很可惜的事情。尽管现如今的深度学习表现出了惊人的效率和准确率，但是Bayesian方法不应被忘记，因为不同于深度学习糟糕的可解释性，Bayesian方法是有良好的数学基础保证的。
{: .message }

## 介绍

你可能听说过，机器学习曾被划分为两大门派：频率学派和贝叶斯学派。在网上你可以找到很多关于这两个学派的解释。

我们用$$\theta$$代表一个机器学习中的模型，用$$(X,Y)$$代表训练数据集，用$$(X^*,Y^*)$$代表任给的测试数据集。那么机器学习问题可以概括为：找到最好的那个$$\theta$$。什么是最好？如果可以做到$$P(Y^*\mid X,\theta)=1$$，那当然是最好的。但是我们不是上帝，所以我们只能做到$$P(Y^*\mid X,\theta)\approx 1$$，即找到一个$$\theta$$，使得$$P(Y^*\mid X,\theta)$$尽可能大。

如何做到这一点？首先我们来做这样一个变换：

$$P(\theta\mid X,Y)=\frac{P(\theta,X,Y)}{P(X,Y)}=\frac{P(Y\mid X,\theta)P(\theta)P(X)}{P(X,Y)}=\frac{P(Y\mid X,\theta)P(\theta)}{P(Y\mid X)}$$

这很简单，就是一个贝叶斯变换。我们给这个式子中的各项做一下定义：
- $$P(\theta\mid X,Y)$$：后验概率 (Posterior Distribution)
- $$P(Y\mid X,\theta)$$：似然函数 (Likelihood Function)
- $$P(\theta)$$：先验概率 (Prior Distribution)
- $$P(Y\mid X)$$：归一化因子 (Normalization Factor)

### 贝叶斯学派

贝叶斯学派认为，人类的知识是有限的, 我们不知道上帝的安排, 就先假设一个先验(我们已有的知识), 再根据训练数据或抽样数据, 去找到后验分布, 就能知道模型最可能是个什么样子。这里面涉及如下逻辑：

- 首先，Bayesian方法中，模型自始至终都不是固定的，而是模型的分布，这个分布由$$\theta$$的分布决定（模型与$$\theta$$的关系由专家给出）。一开始是$$p(\theta)$$，然后伴随着训练的结束，得到$$p(\theta\mid X,Y)$$。
- 一个分布可以生成另一个分布。比如，Beta分布是Bernoulli分布的共轭先验分布。我们以抛硬币为例。不过，我们并不假设硬币是均匀的（正反概率不是$$0.5:0.5$$），所以抛硬币的正面朝上的概率p是未知的（只知道p∈[0,1]）。如果进行一次二项分布试验，在这次二项分布试验中，抛硬币10000次，其中正面朝上7000次，反面朝上3000次，我们可以得到，正负面朝上的概率分别为{p,1-p}={0.7,0.3}。但是我们并不确信这个结果是正确的。我们做10000次二项分布试验，在每次二项分布试验中，均抛硬币10000次（说不定在其他二项分布实验中，得到的正负面朝上的概率是{0.2,0.8}或者{0.6,0.4}，这些情况都有可能），那么，我们想要知道，在这样的多次重复二项分布实验中，抛硬币最后得到正负面朝上概率为{0.7,0.3}这样概率为多少？这就是在求抛硬币的概率分布之上的分布。这样的分布就叫做Beta分布。反过来，我们知道了Beta分布的形式之后就可以从这个Beta分布出发构造出无数个Bernoulli分布（因为Beta分布的随机变量就是Bernoulli分布的$$p$$）。

理清了这两条逻辑，Bayesian方法的思路就清晰了。还是以上面扔硬币的实验作为例子，我们有的数据就是100组实验，每组实验丢硬币10000次。专家告诉我们，丢硬币的模型是一个参数为$$\theta$$的Bernoulli分布，而$$\theta$$服从一个分布$$p(\theta)$$。我们从这些先验的信息出发在数据集中训练得到后验分布$$P(\theta\mid X,Y)$$。进而可以构造Bernoulli分布的分布，即模型的分布。

> 其实上面举的这个扔硬币的例子与我们的公式不太吻合。因为扔硬币问题不是一个Inference问题，所以数据没有X和Y的区分。但是，我们可以把X和Y看成是一组数据，这组数据的概率分布就是我们要求的模型的分布。

### 频率学派

频率学派与贝叶斯学派不同。频率学派认为，模型应该是一个固定的模型（给定一个确定的$$\theta$$，就可以得到一个确定的模型），理想的目标模型的参数已经被上帝指定好了并且固定了，我们也知道这个模型的结构了，现在只需要知道模型的参数就行了。我们的任务就是从数据中学习出这个模型的参数$$\theta$$，使得这个参数尽量接近上帝指定的参数。这个参数$$\theta$$就是我们要求的模型的参数。所以”频率学派”预测投硬币正反的概率的方法就是, 投10000次硬币吧, 看看正面出现多少次。

### Discussion

我们可以看到，频率学派和贝叶斯学派的思路是不一样的。频率学派认为模型是固定的，我们只需要找到模型的参数。而贝叶斯学派认为模型是不固定的，我们需要找到模型的分布。这两种思路的不同，导致了两种学派的方法也是不一样的。频率学派的方法是MLE，贝叶斯学派的方法是MAP。这两种方法的不同，导致了两种学派的结果也是不一样的。频率学派的结果是一个确定的参数，而贝叶斯学派的结果是一个分布。

这两种学派哪个更科学呢？我觉得这个问题没有定论，但是我个人更倾向于贝叶斯学派。因为频率学派有一些无法解决的问题：首先它指定了一个固定概率, 如果上帝指定的模型不是固定的呢；另外, 如果数据量不足够大, 预测会不会非常不准确呢，如果模型参数是1000维的，而数据只有900条，此时频率学派的方法就无法使用，而贝叶斯学派可以继续使用。

可惜的是，贝叶斯学派的方法比较复杂，相较于可以简单快速实现的频率学派的方法，贝叶斯学派的方法需要更多的计算量。所以，贝叶斯学派的方法在实际应用中并不是很常见。但是，贝叶斯学派的方法在理论上是完备的，而频率学派的方法在理论上是不完备的。

## 贝叶斯机器学习方法

哦，这个很难枚举完全，广义上说，只要机器学习得到的是一个模型的分布而不是一个确定的模型，那么这个方法就有贝叶斯学派的影子。下面我将列举一些我在博客里整理过的基于贝叶斯学派的方法。

但是在深入了解贝叶斯机器学习方法之前，我们先来看看要实现贝叶斯机器学习方法，我们需要解决哪些问题。还是熟悉的那个式子：

$$P(\theta\mid X,Y)=\frac{P(\theta,X,Y)}{P(X,Y)}=\frac{P(Y\mid X,\theta)P(\theta)P(X)}{P(X,Y)}=\frac{P(Y\mid X,\theta)P(\theta)}{P(Y\mid X)}$$

对于这个式子，我们想要求解的是$$P(\theta\mid X,Y)$$，于是就需要考虑这么几个问题：

1. 如何求解$$P(Y\mid X,\theta) \Rightarrow$$专家指定了参数到模型的映射，所以这一项是可以计算的。
2. 如何求解$$P(\theta) \Rightarrow$$这一项是先验分布，我们需要指定一个先验分布。
3. 如何求解$$P(Y|X) \Rightarrow$$这一项是归一化因子，实际上描述的是数据的分布情况。最简单的情况下我们可以这样计算：$$P(Y\mid X)=\int P(Y\mid X,\theta)P(\theta)d\theta$$。但是显然这个积分运算几乎无法代码实现。

众多的贝叶斯机器学习方法初衷都是在解决上面提到的问题：

- 贝叶斯网络 (Bayesian Neural Network)
- 高斯过程 (Gaussian Process)
- Monte Carlo Dropout (MC Dropout)